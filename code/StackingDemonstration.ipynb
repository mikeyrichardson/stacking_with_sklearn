{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, Ridge\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    train    = pd.read_csv('../input/train.csv')\n",
    "    test     = pd.read_csv('../input/test.csv')\n",
    "    labels   = train.Hazard.values\n",
    "    test_indices = test.ix[:,'Id']\n",
    "    train.drop('Hazard', axis=1, inplace=True)\n",
    "    train.drop('Id', axis=1, inplace=True)\n",
    "    test.drop('Id', axis=1, inplace=True)\n",
    "    train = np.array(train)\n",
    "    test = np.array(test)\n",
    "    for i in range(train.shape[1]):\n",
    "        if type(train[1,i]) is str:\n",
    "            lbl = LabelEncoder()\n",
    "            lbl.fit(list(train[:,i]) + list(test[:,i]))\n",
    "            train[:,i] = lbl.transform(train[:,i])\n",
    "            test[:,i] = lbl.transform(test[:,i])\n",
    "    return train.astype(float), labels, test.astype(float), test_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, X_test, test_indices = prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some of the features in the data are categorical, it is important to do one hot encoding of the data for certain classifiers. Since the features are anonymous, it's possible that all of the features a categorical. For example, one of the features has values of 1, 2, or 3, but those could refer to something categorical (like Region 1, Region 2, and Region 3) or something quantitative (like number of floors in the house).\n",
    "\n",
    "In this notebook, I'll apply one hot encoding to all of the features since none of them are obviously quantitative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder()\n",
    "encoder.fit(np.vstack([X, X_test]))\n",
    "X_oh = encoder.transform(X)\n",
    "X_test_oh = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use many different classifiers for the first round of predictions. I will divide them into classifiers that need one hot encoded data and those that do not. You might want to add some code to have each models select the best parameters based on some cross validation, but I'll leave that out here. The n_jobs=-1 parameter will ensure that the models use all available processors. \n",
    "\n",
    "For the Ridge and Lasso models, I pass in a cv parameter that tells the model to take the training data and split it up into folds for cross validation. This will help the model choose the best parameters. So for RidgeCV and LassoCV, they are taking the 4 folds that are passed in and using those as the training set. Then they take that training set and split it up into 3 folds. With 2 of those folds, they will train the model using different values of the regularization parameter. Once all the training is done, they can see which value of the regularization parameter produced the best model (by checking the predictions on the left out 3rd fold). The process is repeated leaving out a different fold until all of the folds have been left out. The best regularization parameter overall across all the folds is chosen and then the model is trained on the entire training set using that parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = [('rfr', RandomForestRegressor(n_jobs=-1, max_depth=10)),\n",
    "        ('xtr', ExtraTreesRegressor(n_jobs=-1, max_depth=10)),\n",
    "        ('xgb', XGBRegressor())]\n",
    "\n",
    "clfs_oh = [('rlr', RidgeCV(cv=3, alphas=[0.001, 0.01, 0.1, 1, 10])),\n",
    "           ('llr', LassoCV(cv=3, alphas=[0.001, 0.01, 0.1, 1, 10]))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For stacking, we will first generate an intermediate set of data consisting of the predictions of the labels for the test set. Then we will train a model to combine these intermediate predictions into a final prediction. \n",
    "\n",
    "The tricky part is figuring out how to train the model that uses the intermediate predictions. In order to train, there must be some intermediate data for which the labels are known. We solve this problem in one of two ways:\n",
    "\n",
    "1. Set aside some of the training data and make sure it isn't used when training the first round of models. After all of those models are trained, we can make intermediate predictions on both the unused portion of the training set and the test set. Unfortunately, we have not used all of the training data when training the first round of models. The next approach does use all of the training data. \n",
    "\n",
    "2. Divide the training data into 5 different folds. Train the initial round of models using 4 of the 5 folds and use those trained models to make intermediate predictions for the unused fold. So far, this seems like exactly the same thing we did in the first approach. Here is the difference. Do the same thing all over again holding out a different fold and make intermediate predictions on the left out fold. Continue this until you have ended up training a set of models 5 different times (once for each of the held out folds). Now you have a set of intermediate data that is the same size as the original training set (once you combine all of the intermediate pieces together).\n",
    "\n",
    "The approach I'll use is the second. One question you might have is how we will generate the intermediate data for the test set. We will be training our suite of models 5 different times. Which of those times will we use to generate the intermediate data for the test set? The answer is that we will make intermediate predictions for all of the test set 5 different times and average the intermediate predictions together so that we end up with a single set of intermediate predictions for the test set.\n",
    "\n",
    "In what follows, `X` and `X_oh` will refer to the training set, `X_test` and `X_test_oh` will refer to the test set, `y` will refer to the known labels for the training set, and `test_indices` will refer to the IDs of the test set (used for matching my predictions with the actual labels on the Kaggle server)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rfr model\n",
      "Training xtr model\n",
      "Training xgb model\n",
      "Training rlr model\n",
      "Training llr model\n"
     ]
    }
   ],
   "source": [
    "n_folds = 5\n",
    "splits = KFold(X.shape[0], n_folds=n_folds, random_state=1783)\n",
    "\n",
    "n_classifiers = len(clfs) + len(clfs_oh)\n",
    "X_intermediate = np.zeros((X.shape[0], n_classifiers))\n",
    "X_test_intermediate = np.zeros((X_test.shape[0], n_classifiers))\n",
    "# an X_test_temp array to hold the predictions for the test set \n",
    "# before averaging the results of training with each differents\n",
    "# set of folds\n",
    "X_test_temp = np.zeros((X_test.shape[0], n_folds)) \n",
    "for i, (name, model) in enumerate(clfs):\n",
    "    print 'Training {0} model'.format(name)\n",
    "    for j, (ix_train, ix_hold_out) in enumerate(splits):\n",
    "        model.fit(X[ix_train], y[ix_train])\n",
    "        X_intermediate[ix_hold_out, i] = model.predict(X[ix_hold_out])\n",
    "        X_test_temp[:, j] = model.predict(X_test)\n",
    "    X_test_intermediate[:, i] = X_test_temp.mean(axis=1)\n",
    "\n",
    "offset = len(clfs)\n",
    "for i, (name, model) in enumerate(clfs_oh):\n",
    "    print 'Training {0} model'.format(name)\n",
    "    for j, (ix_train, ix_hold_out) in enumerate(splits):\n",
    "        model.fit(X_oh[ix_train], y[ix_train])\n",
    "        X_intermediate[ix_hold_out, i + offset] = model.predict(X_oh[ix_hold_out])\n",
    "        X_test_temp[:, j] = model.predict(X_test_oh)\n",
    "    X_test_intermediate[:, i + offset] = X_test_temp.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we should check to see how accurately each of the models performed. In this competition, the performance metric was the Gini Index (which is a measure of the accuracy of the relative order of the predicted values instead of the accuracy of the predictions themselves)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized_gini(y_true, y_pred):\n",
    "    # check and get number of samples\n",
    "    assert y_true.shape == y_pred.shape\n",
    "    n_samples = y_true.shape[0]\n",
    "    \n",
    "    # sort rows on prediction column \n",
    "    # (from largest to smallest)\n",
    "    arr = np.array([y_true, y_pred]).transpose()\n",
    "    true_order = arr[arr[:,0].argsort()][::-1,0]\n",
    "    pred_order = arr[arr[:,1].argsort()][::-1,0]\n",
    "    \n",
    "    # get Lorenz curves\n",
    "    L_true = np.cumsum(true_order) / np.sum(true_order)\n",
    "    L_pred = np.cumsum(pred_order) / np.sum(pred_order)\n",
    "    L_ones = np.linspace(0, 1, n_samples)\n",
    "    \n",
    "    # get Gini coefficients (area between curves)\n",
    "    G_true = np.sum(L_ones - L_true)\n",
    "    G_pred = np.sum(L_ones - L_pred) \n",
    "    \n",
    "    # normalize to true Gini coefficient\n",
    "    return G_pred/G_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gini index for rfr: 0.329337945294\n",
      "Gini index for xtr: 0.32964940103\n",
      "Gini index for xgb: 0.368957427389\n",
      "Gini index for rlr: 0.350814296872\n",
      "Gini index for llr: 0.351772339372\n"
     ]
    }
   ],
   "source": [
    "for i, (name, model) in enumerate(clfs + clfs_oh):\n",
    "    print 'Gini index for {0}: {1}'.format(name, normalized_gini(y, X_intermediate[:,i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check the correlation between the predictions of the various models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rfr</th>\n",
       "      <th>xtr</th>\n",
       "      <th>xgb</th>\n",
       "      <th>rlr</th>\n",
       "      <th>llr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rfr</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.808450</td>\n",
       "      <td>0.795750</td>\n",
       "      <td>0.709196</td>\n",
       "      <td>0.718194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xtr</th>\n",
       "      <td>0.808450</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.832333</td>\n",
       "      <td>0.746259</td>\n",
       "      <td>0.759787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.795750</td>\n",
       "      <td>0.832333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.906242</td>\n",
       "      <td>0.914116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rlr</th>\n",
       "      <td>0.709196</td>\n",
       "      <td>0.746259</td>\n",
       "      <td>0.906242</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llr</th>\n",
       "      <td>0.718194</td>\n",
       "      <td>0.759787</td>\n",
       "      <td>0.914116</td>\n",
       "      <td>0.993958</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          rfr       xtr       xgb       rlr       llr\n",
       "rfr  1.000000  0.808450  0.795750  0.709196  0.718194\n",
       "xtr  0.808450  1.000000  0.832333  0.746259  0.759787\n",
       "xgb  0.795750  0.832333  1.000000  0.906242  0.914116\n",
       "rlr  0.709196  0.746259  0.906242  1.000000  0.993958\n",
       "llr  0.718194  0.759787  0.914116  0.993958  1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(X_intermediate, columns=zip(*(clfs + clfs_oh))[0])\n",
    "df_test = pd.DataFrame(X_test_intermediate, columns=zip(*(clfs + clfs_oh))[0])\n",
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the Ridge and Lasso models have almost perfect correlation. You might want to remove one of them from the ensemble since it is unlikely to offer new information while training. I'll leave all of them in.\n",
    "\n",
    "Now all of the intermediate data is created and stored in `X_intermediate` and `X_test_intermediate`. All that's left to do is train a new model on this data. Since you might want to check how much stacking has improved the accuracy or continue on with another level of stacking, I'll first get predictions by the same method used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combinerModel = RidgeCV(cv=3, alphas=[0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10, 30, 100, 300, 1000])\n",
    "X_second_stage = np.zeros((X.shape[0], ))\n",
    "# use splits and X_test_temp from earlier in the code\n",
    "for j, (ix_train, ix_hold_out) in enumerate(splits):\n",
    "    combinerModel.fit(X_intermediate[ix_train], y[ix_train])\n",
    "    X_second_stage[ix_hold_out] = combinerModel.predict(X_intermediate[ix_hold_out])\n",
    "    X_test_temp[:, j] = combinerModel.predict(X_test_intermediate)\n",
    "X_test_second_stage = X_test_temp.mean(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regularization parameter chosen for Ridge model is 30\n",
      "Gini index for combined model: 0.370934145549\n"
     ]
    }
   ],
   "source": [
    "print 'Regularization parameter chosen for Ridge model is {0}'.format(combinerModel.alpha_)\n",
    "print 'Gini index for combined model: {0}'.format(normalized_gini(y, X_second_stage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the stacking has slightly improved the performance of the best model (xgboost) even though the other models were all weaker than xgboost.\n",
    "\n",
    "At this point, we could train a few more models using the intermediate data and then continue to a second round of stacking, but I'll just finish here and use the discovered regularization parameter above to retrain using all of the intermediate data and then make final predictions. (It's possible that RidgeCV retrains with all of the data by default, but I couldn't find any mention of it in the documentation, so I'll just retrain to be safe.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "finalModel = Ridge()\n",
    "finalModel.fit(X_intermediate, y)\n",
    "X_test_predictions = combinerModel.predict(X_test_intermediate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = {}\n",
    "submission['Id'] = test_indices.astype(int)\n",
    "submission['Hazard'] = X_test_predictions\n",
    "df = pd.DataFrame(submission, columns=['Id', 'Hazard'])\n",
    "df.to_csv('../submissions/Stackingv01s01.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
